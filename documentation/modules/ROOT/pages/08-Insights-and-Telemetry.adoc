= Insights and Telemetry
include::_attributes.adoc[]

On an OpenShift Cluster, there are certain services, such as Insight or Telemetry, which require Internet connectivity to function properly. These services continuously send information to the Red Hat Cloud Portal. However, as our cluster is currently disconnected from the Internet, we need to find a workaround to upload this information in a controlled manner. Therefore, we have several options to resolve this disconnected situation:

. Disable both services
. Manual upload (jobs sending info via proxy)
. Using a cluster-wide proxy server

Let's see how to do it in both services. We will start with Insight Service.

[#insights]
== Insights

[#disableinsights]
=== Disable Insights

- Download the global cluster pull secret to your local file system:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc extract secret/pull-secret -n openshift-config --to=.
----

- Edit file `.dockerconfigjson` (make a copy before), and remove the `cloud.openshift.com` JSON entry. 
+
NOTE: Take in account that in our disconnected cluster we have the global secret configured only with the credentials of the private registry. So, in this case, we don't have to modify anything. :) 

- Update global secret:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=.dockerconfigjson
----

[#insightsmanualupload]
=== Manual upload (job via proxy)

For this option, we will generate the info/data and then upload it to the Red Hat Cloud services. However, in order to do so, we require a proxy server or another machine with Internet access. In our specific case, we will create and utilize a proxy. To accomplish this, we will run a container within the Registry VM that will host the proxy server.


- Run a proxy in the registry VM ( access to the registry VM using `kcli ssh registry`` and become root `sudo su` before)
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman run -d --name squid-container -e TZ=UTC -p 3128:3128 docker.io/ubuntu/squid:latest
----

- Exit from Registry VM and stay in the Hypervisor.

- Extract token from pull secret for `cloud.redhat.com`.
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat ~/pull-secret-all.json | jq '.auths["cloud.openshift.com"].auth'
----

- Extract clusterid from cluster:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get clusterversion -o jsonpath='{.items[].spec.clusterID}{"\n"}'
----

- Create job and update environment variables TOKEN, CLUSTERID and HTTPS_PROXY properly with the above values (`8-insights-cronjob.yaml`)
+
[.console-input]
[source,yaml,subs="+macros,+attributes"]
----
include::example$8-insights-cronjob.yaml[]
----

- and "install" the cronjob:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc apply -f 8-insights-cronjob.yaml
----
+
[NOTE]
====
This is a cronjob running every 30 minutes. You can change the schedule expression to run again the job. Alternatively, you can create a job based on this cronjob executing:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc create job -n openshift-insights --from=cronjob.batch/insights-operator-cronjob test
----
====


[#telemetry]
== Telemetry

The telemetry service/client is a component of Openshift Monitoring. For this service there are also the same 3 options:

- Disable: Same way as Insight, removing credentials from global pull-secret. (We won't cover this method)
- Manual upload (job via proxy)
- Using a cluster-wide proxy


[#telemetrymanualupload]
=== Manual Upload

Actually, it's an "unsupported" way of forcing sending metrics data.

[IMPORTANT]
====
This service is only installed if in the global pull-secret are the credentials for `cloud.redhat.com`. So, if in your cluster is not configured, added it again.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=./pull-secret-all.json
----
====

Once added, wait some minutes, or force deletion of the operator pod to reconcile, and it can (re)create the telemeter-client pod.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc delete pod -l app=cluster-monitoring-operator -n openshift-monitoring
----

Wait until telemeter pod is created:

[.console-output]
----
$ oc get pod -n openshift-monitoring | grep telemeter-client
telemeter-client-85d9d5fcbc-49chl                        3/3     Running   0          16s
----

To enable the telemeter pod to use the proxy server for sending data, we simply need to add the HTTPS_PROXY variable to the deployment. This can be accomplished by executing the following command:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc set env -n openshift-monitoring deployment/telemeter-client -c telemeter-client HTTPS_PROXY=registry.dsal:3128
----

When Monitoring Operator reconciles the changes will be lost, so we can execute this command in a k8s cronjob or externally on demand.

To check the status of insight and telemetry, browse in `https://console.redhat.com/openshift`
